{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Détection et analyse des visages\r\n",
        "\r\n",
        "Les solutions de vision par ordinateur requièrent souvent une solution d’intelligence artificielle (IA) pour pouvoir détecter, analyser ou identifier des visages humains. Supposons par exemple que l’entreprise de vente au détail Northwind Traders ait décidé de mettre en place un « magasin intelligent », dans lequel les services d’IA surveillent le magasin pour identifier les clients ayant besoin d’aide et envoient les employés concernés pour les aider. L’un des moyens d’y parvenir est de procéder à la détection et à l’analyse des visages. En d’autres termes, de déterminer s’il y a des visages sur les images et, le cas échéant, d’analyser leurs caractéristiques.\r\n",
        "\r\n",
        "![Robot analysant un visage](./images/face_analysis.jpg)\r\n",
        "\r\n",
        "## Utilisez le service cognitif Visage pour détecter les visages\r\n",
        "\r\n",
        "Supposons que le système de magasin intelligent que Northwind Traders souhaite créer doive être capable de détecter les clients et d’analyser leurs caractéristiques faciales. Pour y parvenir, dans Microsoft Azure, vous pouvez utiliser la fonction **Visage**, qui fait partie d’Azure Cognitive Services.\r\n",
        "\r\n",
        "### Créer une ressource Cognitive Services\r\n",
        "\r\n",
        "Commencez par créer une ressource **Cognitive Services** dans votre abonnement Azure.\r\n",
        "\r\n",
        "> **Remarque** : Si vous disposez déjà d’une ressource Cognitive Services, il suffit d’ouvrir sa page **Démarrage rapide** dans le portail Azure et de copier sa clé et son point de terminaison dans la cellule ci-dessous. Sinon, suivez les étapes ci-dessous pour en créer une.\r\n",
        "\r\n",
        "1. Sous un autre onglet du navigateur, ouvrez le portail Azure à l’adresse https://portal.azure.com, en vous connectant avec votre compte Microsoft.\r\n",
        "2. Cliquez sur le bouton **&#65291; Créer une ressource**, recherchez *Cognitive Services*, puis créez une ressource **Cognitive Services** avec les paramètres suivants :\r\n",
        "    - **Abonnement** : *Votre abonnement Azure*.\r\n",
        "    - **Groupe de ressources** : *Sélectionnez ou créez un groupe de ressources portant un nom unique*.\r\n",
        "    - **Région** : *Choisissez une région disponible* :\r\n",
        "    - **Nom** : *Saisissez un nom unique*.\r\n",
        "    - **Niveau tarifaire** : S0\r\n",
        "    - **Je confirme avoir lu et compris les avis** : Sélectionné.\r\n",
        "3. Attendez la fin du déploiement. Ensuite, accédez à votre ressource Cognitive Services et, sur la page **Aperçu**, cliquez sur le lien permettant de gérer les clés du service. Vous aurez besoin du point de terminaison et des clés pour vous connecter à votre ressource Cognitive Services à partir des applications clientes.\r\n",
        "\r\n",
        "### Obtenir la clé et le point de terminaison de votre ressource Cognitive Services\r\n",
        "\r\n",
        "Pour utiliser votre ressource Cognitive Services, les applications clientes ont besoin de son point de terminaison et de sa clé d’authentification :\r\n",
        "\r\n",
        "1. Dans le portail Azure, sur la page **Clés et Point de terminaison** de votre ressource Cognitive Services, copiez la **Clé 1** de votre ressource et collez-la dans le code ci-dessous, en remplaçant **la valeur YOUR_COG_KEY**.\r\n",
        "\r\n",
        "2. Copiez le **point de terminaison** de votre ressource et collez-le dans le code ci-dessous, en remplaçant la valeur **YOUR_COG_ENDPOINT**.\r\n",
        "\r\n",
        "3. Exécutez le code dans la cellule ci-dessous en cliquant sur le bouton Exécuter la cellule <span>&#9655;</span> (en haut à gauche de la cellule)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693964655
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que vous avez une ressource Cognitive Services, vous pouvez utiliser le service Visage pour détecter les visages humains dans le magasin.\r\n",
        "\r\n",
        "Exécutez la cellule de code ci-dessous pour voir un exemple."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from python_code import faces\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Create a face detection client.\r\n",
        "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Open an image\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detect faces\r\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# Display the faces (code in python_code/faces.py)\r\n",
        "faces.show_faces(image_path, detected_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693970079
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un ID unique est attribué à chaque visage détecté, de sorte que votre application peut identifier chaque visage individuel détecté.\r\n",
        "\r\n",
        "Exécutez la cellule ci-dessous pour voir les ID de quelques visages de clients supplémentaires."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Open an image\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detect faces\r\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# Display the faces (code in python_code/faces.py)\r\n",
        "faces.show_faces(image_path, detected_faces, show_id=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693970447
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyser les attributs du visage\r\n",
        "\r\n",
        "La fonctionnalité Visage ne se limite pas à détecter des visages. Elle peut également analyser les caractéristiques et les expressions du visage pour suggérer l’âge et l’état émotionnel. Par exemple, exécutez le code ci-dessous pour analyser les attributs du visage d’un client."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Open an image\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam1.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detect faces and specified facial attributes\r\n",
        "attributes = ['age', 'emotion']\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
        "\n",
        "# Display the faces and attributes (code in python_code/faces.py)\r\n",
        "faces.show_face_attributes(image_path, detected_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693971321
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "D’après les scores d’émotion détectés pour le client dans l’image, celui-ci semble plutôt satisfait de son expérience d’achat.\r\n",
        "\r\n",
        "## Rechercher des visages semblables \r\n",
        "\r\n",
        "Les ID de visage qui sont créés pour chaque visage détecté sont utilisés pour identifier individuellement les détections de visage. Vous pouvez utiliser ces ID pour comparer un visage détecté à des visages précédemment détectés et trouver des visages présentant des caractéristiques similaires.\r\n",
        "\r\n",
        "Par exemple, exécutez la cellule ci-dessous pour comparer le client d’une image avec les clients d’une autre image et trouver un visage correspondant."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the ID of the first face in image 1\r\n",
        "image_1_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
        "image_1_stream = open(image_1_path, \"rb\")\n",
        "image_1_faces = face_client.face.detect_with_stream(image=image_1_stream)\n",
        "face_1 = image_1_faces[0]\n",
        "\n",
        "# Get the face IDs in a second image\r\n",
        "image_2_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
        "image_2_stream = open(image_2_path, \"rb\")\n",
        "image_2_faces = face_client.face.detect_with_stream(image=image_2_stream)\n",
        "image_2_face_ids = list(map(lambda face: face.face_id, image_2_faces))\n",
        "\n",
        "# Find faces in image 2 that are similar to the one in image 1\r\n",
        "similar_faces = face_client.face.find_similar(face_id=face_1.face_id, face_ids=image_2_face_ids)\n",
        "\n",
        "# Show the face in image 1, and similar faces in image 2(code in python_code/face.py)\r\n",
        "faces.show_similar_faces(image_1_path, face_1, image_2_path, image_2_faces, similar_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693972555
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconnaître des visages\r\n",
        "\r\n",
        "Jusqu’à présent, vous avez vu que la fonction Visage pouvait détecter des visages et des caractéristiques faciales et même identifier deux visages similaires. Vous pouvez aller plus loin en complétant une solution de *reconnaissance faciale* dans laquelle vous entraînez la fonctionnalité Visage à reconnaître le visage d’une personne spécifique. Cela peut être utile dans divers scénarios, comme l’étiquetage automatique des photos d’amis dans une application de réseaux sociaux, ou l’utilisation de la reconnaissance faciale dans le cadre d’un système de vérification d’identité biométrique.\r\n",
        "\r\n",
        "Pour voir comment cela fonctionne, supposons que la société Northwind Traders souhaite utiliser la reconnaissance faciale pour s’assurer que seuls les employés autorisés du département informatique peuvent accéder aux systèmes sécurisés.\r\n",
        "\r\n",
        "Nous allons commencer par créer un *groupe de personnes* pour représenter les employés autorisés."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "group_id = 'employee_group_id'\n",
        "try:\n",
        "    # Delete group if it already exists\n",
        "    face_client.person_group.delete(group_id)\n",
        "except Exception as ex:\n",
        "    print(ex.message)\n",
        "finally:\n",
        "    face_client.person_group.create(group_id, 'employees')\n",
        "    print ('Group created!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693973492
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que le *groupe de personnes* existe, nous pouvons tout d’abord ajouter une *personne* pour chaque employé que nous voulons inclure dans le groupe. Nous procéderons ensuite à l’enregistrement de plusieurs photographies de chaque personne afin que la fonctionnalité Visage puisse reconnaître les caractéristiques faciales distinctes de chaque personne. Idéalement, les images devraient montrer la même personne dans différentes poses et avec différentes expressions faciales.\r\n",
        "\r\n",
        "Nous allons ajouter un seul employé, dénommé Wendell, puis enregistrer trois photographies de l’employé."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Add a person (Wendell) to the group\r\n",
        "wendell = face_client.person_group_person.create(group_id, 'Wendell')\n",
        "\n",
        "# Get photo's of Wendell\r\n",
        "folder = os.path.join('data', 'face', 'wendell')\n",
        "wendell_pics = os.listdir(folder)\n",
        "\n",
        "# Register the photos\r\n",
        "i = 0\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "for pic in wendell_pics:\n",
        "    # Add each photo to person in person group\n",
        "    img_path = os.path.join(folder, pic)\n",
        "    img_stream = open(img_path, \"rb\")\n",
        "    face_client.person_group_person.add_face_from_stream(group_id, wendell.person_id, img_stream)\n",
        "\n",
        "    # Display each image\n",
        "    img = Image.open(img_path)\n",
        "    i +=1\n",
        "    a=fig.add_subplot(1,len(wendell_pics), i)\n",
        "    a.axis('off')\n",
        "    imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693976898
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une fois la personne ajoutée et les photographies enregistrées, nous pouvons maintenant entraîner la fonctionnalité Visage à reconnaître chaque personne."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "face_client.person_group.train(group_id)\n",
        "print('Trained!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693977046
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, une fois le modèle formé, vous pouvez l’utiliser pour identifier les visages reconnus dans une image."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the face IDs in a second image\r\n",
        "image_path = os.path.join('data', 'face', 'employees.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "image_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "image_face_ids = list(map(lambda face: face.face_id, image_faces))\n",
        "\n",
        "# Get recognized face names\r\n",
        "face_names = {}\n",
        "recognized_faces = face_client.face.identify(image_face_ids, group_id)\n",
        "for face in recognized_faces:\n",
        "    person_name = face_client.person_group_person.get(group_id, face.candidates[0].person_id).name\n",
        "    face_names[face.face_id] = person_name\n",
        "\n",
        "# show recognized faces\r\n",
        "faces.show_recognized_faces(image_path, image_faces, face_names)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693994820
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## En savoir plus\r\n",
        "\r\n",
        "Pour en savoir plus sur le service cognitif Visage, consultez la [documentation Visage](https://docs.microsoft.com/azure/cognitive-services/face/)\r\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}