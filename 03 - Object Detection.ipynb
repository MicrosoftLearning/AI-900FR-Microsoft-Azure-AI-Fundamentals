{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Détection d’objets\r\n",
        "\r\n",
        "*Détection d’objets* est une forme de vision informatique dans laquelle un modèle d’apprentissage automatique est entraîné à classer les instances individuelles d’objets dans une image et à indiquer une *case de délimitation* qui marque son emplacement. Vous pouvez considérer qu’il s’agit d’une progression de la *classification d’images* (dans laquelle le modèle répond à la question « de quoi s’agit-il ? ») vers la construction de solutions où l’on peut demander au modèle « quels sont les objets présents dans cette image, et où sont-ils ? ».\r\n",
        "\r\n",
        "![Un robot identifiant un fruit](./images/object-detection.jpg)\r\n",
        "\r\n",
        "Par exemple, une épicerie pourrait utiliser un modèle de détection d’objets pour mettre en œuvre un système de caisse automatisé qui scannerait un tapis roulant à l’aide d’une caméra et pourrait identifier des articles spécifiques sans avoir besoin de placer chaque article sur le tapis et de les scanner individuellement.\r\n",
        "\r\n",
        "Le service cognitif **Vision personnalisée** de Microsoft Azure offre une solution basée sur le cloud pour créer et publier des modèles de détection d’objets personnalisés.\r\n",
        "\r\n",
        "## Créer une ressource Vision personnalisée\r\n",
        "\r\n",
        "Pour utiliser le service Vision personnalisée, vous avez besoin d’une ressource Azure que vous pouvez utiliser pour entraîner un modèle, mais aussi d’une ressource avec laquelle vous pouvez le publier pour que les applications puissent l’utiliser. Vous pouvez utiliser la même ressource pour chacune de ces tâches, ou utiliser des ressources différentes pour chacune d’elles afin d’allouer les coûts séparément, à condition que les deux ressources soient créées dans la même région. La ressource pour l’une ou l’autre (ou les deux) tâches peut être une ressource **Cognitive Services** générale ou une ressource **Vision personnalisée** spécifique. Suivez les instructions suivantes pour créer une nouvelle ressource **Vision personnalisée** (ou vous pouvez utiliser une éventuelle ressource existante).\r\n",
        "\r\n",
        "1. Sous un nouvel onglet de navigateur, ouvrez le portail Azure à l’adresse [https://portal.azure.com](https://portal.azure.com) et connectez-vous en utilisant le compte Microsoft associé à votre abonnement Azure.\r\n",
        "2. Sélectionnez le bouton **&#65291; Créer une ressource**, recherchez *Vision personnalisée* et créez une ressource **Vision personnalisée** avec les paramètres suivants :\r\n",
        "    - **Créer des options** : Les deux\r\n",
        "    - **Abonnement** : *Votre abonnement Azure*\r\n",
        "    - **Groupe de ressources** : *Sélectionnez ou créez un groupe de ressources portant un nom unique*\r\n",
        "    - **Nom** : *Entrez un nom unique*\r\n",
        "    - **Emplacement de formation** : *Choisissez une région disponible*\r\n",
        "    **Niveau tarifaire de formation ** : F0\r\n",
        "    - **Emplacement de prédiction** : *Valeur identique à l’emplacement de formation*\r\n",
        "    - **Niveau tarifaire de prédiction** : F0\r\n",
        "\r\n",
        "    > **Remarque** : Si vous avez déjà un service de vision personnalisée F0 dans votre abonnement, sélectionnez **S0** pour celui-ci.\r\n",
        "\r\n",
        "3. Attendez que la ressource soit créée.\r\n",
        "\r\n",
        "## Créer un projet Vision personnalisée\r\n",
        "\r\n",
        "Pour former un modèle de détection d’objets, vous devez créer un projet Vision personnalisée basé sur votre ressource de formation. Pour ce faire, utilisez le portail Vision personnalisée.\r\n",
        "\r\n",
        "1. Sous un nouvel onglet de navigateur, ouvrez le portail Vision personnalisée à l’adresse [https://customvision.ai](https://customvision.ai), puis connectez-vous en utilisant le compte Microsoft associé à votre abonnement Azure.\r\n",
        "2. Créez un nouveau projet avec les paramètres suivants :\r\n",
        "    - **Nom** : Détection Épicerie\r\n",
        "    - **Description** : Détection d’objets pour les épiceries.\r\n",
        "    - **Ressource** : *La ressource Vision personnalisée que vous avez créée précédemment*\r\n",
        "    - **Types de projets** : Détection d’objets\r\n",
        "    - **Domaines** : Général\r\n",
        "3. Attendez que le projet soit créé et ouvert dans le navigateur.\r\n",
        "\r\n",
        "## Ajoutez et étiquetez des images\r\n",
        "\r\n",
        "Pour entraîner un modèle de détection d’objets, vous devez d’abord télécharger des images contenant les classes que le modèle doit identifier. Étiquetez-les ensuite pour indiquer les cases de délimitation de chaque instance d’objet.\r\n",
        "\r\n",
        "1. Téléchargez et extrayez les images de formation à partir de https://aka.ms/fruit-objects. Le dossier extrait contient une collection d’images de fruits.\r\n",
        "2. Dans le portail Vision personnalisée, dans votre projet de détection d’objets, sélectionnez **Ajouter des images** et téléchargez toutes les images du dossier extrait.\r\n",
        "3. Une fois les images téléchargées, sélectionnez la première pour l’ouvrir.\r\n",
        "4. Maintenez la souris sur n’importe quel objet de l’image jusqu’à ce qu’une région détectée automatiquement s’affiche, comme illustré dans l’image ci-dessous. Sélectionnez ensuite l’objet et, si nécessaire, redimensionnez la région pour l’entourer.\r\n",
        "\r\n",
        "![La région par défaut pour un objet](./images/object-region.jpg)\r\n",
        "\r\n",
        "Vous pouvez aussi simplement faire glisser l’objet pour créer une région.\r\n",
        "\r\n",
        "5. Lorsque la région entoure l’objet, ajoutez une nouvelle étiquette avec le type d’objet approprié (*pomme*, *banane*, ou *orange*) comme illustré ici :\r\n",
        "\r\n",
        "![Un objet étiqueté dans une image](./images/object-tag.jpg)\r\n",
        "\r\n",
        "6. Sélectionnez et étiquetez chaque autre objet de l’image, en redimensionnant les régions et en ajoutant de nouvelles étiquettes si nécessaire.\r\n",
        "\r\n",
        "![Deux objets étiquetés dans une image](./images/object-tags.jpg)\r\n",
        "\r\n",
        "7. Utilisez le lien **>** sur la droite pour passer à l’image suivante et étiqueter ses objets. Continuez ensuite à parcourir toute la collection d’images, en étiquetant chaque pomme, banane et orange.\r\n",
        "\r\n",
        "8. Lorsque vous avez terminé d’étiqueter la dernière image, fermez l’éditeur **Détails de l’image** puis, sur la page **Images de formation**, sous **Étiquettes**, sélectionnez **Étiqueté** pour voir toutes vos images étiquetées :\r\n",
        "\r\n",
        "![Images marquées dans un projet](./images/tagged-images.jpg)\r\n",
        "\r\n",
        "## Former et tester un modèle\r\n",
        "\r\n",
        "Maintenant que vous avez étiqueté les images de votre projet, vous êtes prêt à former un modèle.\r\n",
        "\r\n",
        "1. Dans le projet Vision personnalisée, cliquez sur **Former** pour former un modèle de détection d’objets à l’aide des images étiquetées. Sélectionnez l’option **Formation rapide**.\r\n",
        "2. Attendez la fin de la formation (cela peut prendre une dizaine de minutes), puis vérifiez les mesures de performance *Précision*, *Rappel*, et *mAP*. Elles mesurent la précision de prédiction du modèle de classification et doivent toutes être élevées.\r\n",
        "3. En haut à droite de la page, cliquez sur **Test rapide**, puis dans la case **URL image**, entrez `https://aka.ms/apple-orange` et examinez la prédiction générée. Fermez ensuite la fenêtre **Test rapide**.\r\n",
        "\r\n",
        "## Publier et consommer le modèle de détection d’objets\r\n",
        "\r\n",
        "Vous êtes maintenant prêt à publier votre modèle formé et à l’utiliser à partir d’une application cliente.\r\n",
        "\r\n",
        "1. Dans la partie supérieure gauche de la page **Performances**, cliquez sur **&#128504; Publier** pour publier le modèle formé avec les paramètres suivants :\r\n",
        "    - **Nom du modèle** : détecter-produire\r\n",
        "    - **Ressource de prédiction** : *Votre **ressource** de prédiction* de vision personnalisée.\r\n",
        "\r\n",
        "### (!) Vérification \r\n",
        "Avez-vous utilisé le même nom de modèle : **détecter-produire** ? \r\n",
        "\r\n",
        "2. Après la publication, cliquez sur l’icône *Paramètres* (&#9881;) en haut à droite de la page **Performances** pour afficher les paramètres du projet. Ensuite, sous **Généralités** (à gauche), copiez **ID du projet**. Faites défiler la page vers le bas et collez-la dans la cellule de code située sous l’étape 5 en remplaçant la valeur **YOUR_PROJECT_ID**. \r\n",
        "\r\n",
        "> (*(si vous avez utilisé une ressource **Cognitive Services** au lieu de créer une ressource **Vision personnalisée** au début de cet exercice, vous pouvez copier sa clé et son point de terminaison à partir de la partie droite des paramètres du projet, les coller dans la cellule de code ci-dessous et l’exécuter pour voir les résultats. Sinon, continuez à suivre les étapes ci-dessous pour obtenir la clé et le point de terminaison de votre ressource de prédiction Vision personnalisée*).\r\n",
        "\r\n",
        "3. Dans la partie supérieure gauche de la page **Paramètres du projet**, cliquez sur l’icône *Galerie de projets* (&#128065;) pour revenir à la page d’accueil du portail Vision personnalisée, où votre projet est maintenant répertorié.\r\n",
        "\r\n",
        "4. Sur la page d’accueil du portail Vision personnalisée, en haut à droite, cliquez sur l’icône *Paramètres* (&#9881;) pour afficher les paramètres de votre service Vision personnalisée. Ensuite, sous **Ressources**, développez votre ressource *Prédiction* (<u>et non</u> la ressource Formation) et copiez ses valeurs **Clé** et **Point de terminaison** dans la cellule de code, sous l’étape 5, en remplaçant les valeurs **YOUR_KEY** et **YOUR_ENDPOINT**.\r\n",
        "\r\n",
        "### (!) Vérification \r\n",
        "Si vous utilisez une ressource **Vision personnalisée**, avez-vous utilisé la ressource **Prédiction** (<u>et non</u> la ressource Formation) ?\r\n",
        "\r\n",
        "5. Exécutez la cellule de code ci-dessous en cliquant sur le bouton Exécuter la cellule <span>&#9655;</span> (en haut à gauche de la cellule) pour définir les variables sur vos valeurs d’ID de projet, de clé et de point de terminaison."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\n",
        "cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\n",
        "cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\n",
        "\n",
        "model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599692485387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez maintenant utiliser votre clé et votre point de terminaison avec un client Vision personnalisée pour vous connecter à votre modèle de détection d’objets de vision personnalisée.\r\n",
        "\r\n",
        "Exécutez la cellule de code suivante, qui utilise votre modèle pour détecter des produits individuels dans une image.\r\n",
        "\r\n",
        "> **Remarque** : Ne vous souciez pas des détails du code. Il utilise le SDK Python pour le service Vision personnalisée pour soumettre une image à votre modèle et récupérer les prédictions pour les objets détectés. Chaque prédiction se compose d’un nom de classe (*pomme*, *banane*, ou *orange*) et les coordonnées *case de délimitation* qui indiquent où, dans l’image, l’objet prédit a été détecté. Le code utilise ensuite ces informations pour dessiner une boîte étiquetée autour de chaque objet sur l’image."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
        "from msrest.authentication import ApiKeyCredentials\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Charger une image de test et obtenir ses dimensions\r\n",
        "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\n",
        "test_img = Image.open(test_img_file)\n",
        "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\n",
        "\n",
        "# Obtenir un client de prédiction pour le modèle de détection d’objets\r\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\n",
        "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\n",
        "\n",
        "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\n",
        "\n",
        "# Détecter des objets dans l’image de test\r\n",
        "with open(test_img_file, mode=\"rb\") as test_data:\n",
        "    results = predictor.detect_image(project_id, model_name, test_data)\n",
        "\n",
        "# Créer une figure pour afficher les résultats\r\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.axis('off')\n",
        "\n",
        "# Afficher l’image avec des cases autour de chaque objet détecté\r\n",
        "draw = ImageDraw.Draw(test_img)\n",
        "lineWidth = int(np.array(test_img).shape[1]/100)\n",
        "object_colors = {\n",
        "    \"apple\": \"lightgreen\",\n",
        "    \"banana\": \"yellow\",\n",
        "    \"orange\": \"orange\"\n",
        "}\n",
        "for prediction in results.predictions:\n",
        "    color = 'white' # default for 'other' object tags\n",
        "    if (prediction.probability*100) > 50:\n",
        "        if prediction.tag_name in object_colors:\n",
        "            color = object_colors[prediction.tag_name]\n",
        "        left = prediction.bounding_box.left * test_img_w \n",
        "        top = prediction.bounding_box.top * test_img_h \n",
        "        height = prediction.bounding_box.height * test_img_h\n",
        "        width =  prediction.bounding_box.width * test_img_w\n",
        "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\n",
        "        draw.line(points, fill=color, width=lineWidth)\n",
        "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\n",
        "plt.imshow(test_img)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599692585672
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher les prédictions résultantes, qui montrent les objets détectés et la probabilité de chaque prédiction."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}